---

name : "nar"

random_seed : 42

# デバッグ
debug : True
debug_iter : 1

# checkpointから再開する場合
check_point_start : False
start_ckpt_path : "~/lip2sp_pytorch/check_point/nar/face_cropped_max_size_fps25/2023:07:20_02-32-49/mspec80_400.ckpt"

# check_point_startだと、以前の保存用ディレクトリに続けて保存されてしまうので、分けたい時用
check_point_start_separate_save_dir : False
start_ckpt_path_separate_save_dir : "~/lip2sp_pytorch/check_point/nar/face_cropped_max_size_fps25/2023:07:20_02-32-49/mspec80_400.ckpt"

# 使用するデータの種類の指定
face_or_lip : "face_cropped_max_size_fps25"

# 学習したモデルのパラメータを保存するディレクトリまでのパス
save_path : "~/lip2sp_pytorch/result/nar/train"

# check point path
ckpt_path : "~/lip2sp_pytorch/check_point/nar"
ckpt_step : 1

# npzファイルを生成する際の話者リスト
npz_process_speaker_list : []

# 口唇動画、音響特徴量ディレクトリまでのパス
face_cropped_max_size_fps25_train : "~/dataset/lip/np_files/face_cropped_max_size_fps25_0_25_gray/train"
face_cropped_max_size_fps25_val : "~/dataset/lip/np_files/face_cropped_max_size_fps25_0_25_gray/val"

# 使用するコーパス
corpus : ["ATR", "BASIC5000", "balanced"]

# 使用する話者
speaker : ["F01_kablab"]
use_spk_emb : True
where_spk_emb : "after_res"
adversarial_learning : False

# max_epoch
max_epoch : 2

# data augmentationの有無
# 見た目変換系
use_color_jitter : False
use_blur : False
use_pad : False
use_rotation : False
use_horizontal_flip : False
use_random_crop : False

# 空間領域にランダムマスキング
use_spatial_masking : False
which_spatial_mask : "has"
spatial_divide_factor : 8
n_spatial_mask : 24
mask_length : 24

# 再生速度変換
use_time_augment : False
time_augment_rate : 20    # (100 - rate)から(100 + rate)の範囲で再生速度を変更

# 動画の連続したフレームをある程度まとめてマスキング
use_segment_masking : True
which_seg_mask : "seg_mean"
max_segment_masking_sec : 0.5

# 音響特徴量に対してのマスキング
use_time_frequency_masking : False
feature_time_masking_length : 50
feature_freq_masking_band : 40

# dataloader
batch_size : 2
num_workers : 1

# dropout
dec_dropout : 0.1
res_dropout : 0.1
rnn_dropout : 0.1
lm_enc_dropout : 0.1

# optimizer
lr : 0.001
beta_1 : 0.9
beta_2 : 0.999
weight_decay : 1.0e-6
# beta_2 : 0.98
# weight_decay : 1.0e-2
which_optim : 'adam'
which_scheduler : 'exp'

# scheduler
lr_decay_rate : 0.5
multi_lr_decay_step : [200, 400]   # 学習率を変更するepoch
lr_decay_exp : 0.995
use_warmup_scheduler : False
warmup_t_rate : 0.1
warmup_lr_init : 1.0e-5
warmup_lr_min : 0

# gradient clipping
max_norm : 3.0

# lossの重みづけ
mse_weight : 1.0
classifier_weight : 0.05
use_weighted_mean : False
loss_disc_weight : 0.01

# gradient accumulation
iters_to_accumulate : 1