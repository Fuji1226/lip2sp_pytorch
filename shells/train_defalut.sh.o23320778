
  - ç¾åœ¨ã®ç ”ç©¶å®¤ã®GPUä½¿ç”¨çŠ¶æ³ã¯ï¼Œ
    ã€Œonsei_check_gpusã€ã¨ã„ã†ã‚³ãƒžãƒ³ãƒ‰ã§ç¢ºèªã§ãã¾ã™ã€‚
  - è‡ªåˆ†ãŒç¢ºä¿ã—ãŸã„GPUã®æžšæ•°ã¯ï¼Œ
    ã€Œonsei_register_desired_gpusã€ã¨ã„ã†ã‚³ãƒžãƒ³ãƒ‰ã§ç™»éŒ²ã§ãã¾ã™ã€‚
    - GPUã‚’å½“åˆ†ä½¿ã†äºˆå®šãŒãªã„å ´åˆã®ä¾‹ï¼ˆæœŸé™ã‚’9999å¹´12æœˆ31æ—¥ã«è¨­å®šï¼‰
      onsei_register_desired_gpus 0 9999/12/31
    - 8æžšç¢ºä¿ã—ãŸã„å ´åˆã®ä¾‹ï¼ˆæœŸé™ã‚’2022å¹´10æœˆ11æ—¥ã«è¨­å®šã—ï¼Œã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã‚‹ï¼‰
      onsei_register_desired_gpus 8 2022/10/11 'ç ”ç©¶ç· åˆ‡ã®ãŸã‚ã™ã¿ã¾ã›ã‚“'
    - æœ€ä½Ž2æžšç¢ºä¿ã—ï¼Œç ”ç©¶å®¤å…¨ä½“ã§ä½™ã£ãŸGPUã‚‚ä½¿ã„ãŸã„å ´åˆã®ä¾‹
      onsei_register_desired_gpus 2 2022/10/11 'ä½™å‰°GPUã‚’ä½¿ã‚ã›ã¦ã„ãŸã ãã¾ã™'

USER      RUNNING DESIRED     TO DATE  COMMENT
============================================================================
hidaka          0       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
minami          5       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
takehisa        0       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
fujita          6       5  2022/10/31  ä½¿ã„ã¾ã™
============================================================================
Remaining       5

wandb: Currently logged in as: naoaki. Use `wandb login --relogin` to force relogin
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
tag: scale_slen600_start040
device = cuda
cpu_num = 36
gpu_num = 1

--- data directory check ---
data_root = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/train
mean_std_path = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/mean_std
ckpt_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/check_point/default/lip/scale_slen600_start040
save_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/result/default/train/lip/scale_slen600_start040

--- get datasets ---
load F01_kablab

--- make train dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 3639

--- make validation dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 192
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/outputs/2022-10-04/14-55-25/wandb/run-20221004_145526-22hsrj7c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scale_slen600_start040_mspec80
wandb: â­ï¸ View project at https://wandb.ai/naoaki/lip2sp_default
wandb: ðŸš€ View run at https://wandb.ai/naoaki/lip2sp_default/runs/22hsrj7c
##### 1 #####
training_method : ss/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

mixing_prob = 0.5
learning_rate = 0.001
iter start
iter 0/113
Traceback (most recent call last):
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced scale_slen600_start040_mspec80: https://wandb.ai/naoaki/lip2sp_default/runs/22hsrj7c
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221004_145526-22hsrj7c/logs
Error executing job with overrides: ['train.debug=False', 'tag=scale_slen600_start040']
Traceback (most recent call last):
  File "train_default.py", line 351, in main
    train_epoch_loss_output, train_epoch_loss_dec_output, train_epoch_loss_feat_add = train_one_epoch(
  File "train_default.py", line 107, in train_one_epoch
    output, dec_output, feat_add_out = model(lip=lip, prev=feature, data_len=data_len, training_method=training_method, mixing_prob=mixing_prob)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1128, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/model/model_default.py", line 146, in forward
    lip_feature = self.ResNet_GAP(lip) #(B, C, T)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/model/net.py", line 142, in forward
    out = layer(out)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/model/net.py", line 99, in forward
    y1 = self.bn2(y1)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/model/net.py", line 18, in forward
    out = self.b_n(x)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/nn/functional.py", line 2421, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 676.00 MiB (GPU 0; 15.90 GiB total capacity; 14.31 GiB already allocated; 569.75 MiB free; 14.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
