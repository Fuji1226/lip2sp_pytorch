
  - ç¾åœ¨ã®ç ”ç©¶å®¤ã®GPUä½¿ç”¨çŠ¶æ³ã¯ï¼Œ
    ã€Œonsei_check_gpusã€ã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ã§ç¢ºèªã§ãã¾ã™ã€‚
  - è‡ªåˆ†ãŒç¢ºä¿ã—ãŸã„GPUã®æšæ•°ã¯ï¼Œ
    ã€Œonsei_register_desired_gpusã€ã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ã§ç™»éŒ²ã§ãã¾ã™ã€‚
    - GPUã‚’å½“åˆ†ä½¿ã†äºˆå®šãŒãªã„å ´åˆã®ä¾‹ï¼ˆæœŸé™ã‚’9999å¹´12æœˆ31æ—¥ã«è¨­å®šï¼‰
      onsei_register_desired_gpus 0 9999/12/31
    - 8æšç¢ºä¿ã—ãŸã„å ´åˆã®ä¾‹ï¼ˆæœŸé™ã‚’2022å¹´10æœˆ11æ—¥ã«è¨­å®šã—ï¼Œã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã‚‹ï¼‰
      onsei_register_desired_gpus 8 2022/10/11 'ç ”ç©¶ç· åˆ‡ã®ãŸã‚ã™ã¿ã¾ã›ã‚“'
    - æœ€ä½2æšç¢ºä¿ã—ï¼Œç ”ç©¶å®¤å…¨ä½“ã§ä½™ã£ãŸGPUã‚‚ä½¿ã„ãŸã„å ´åˆã®ä¾‹
      onsei_register_desired_gpus 2 2022/10/11 'ä½™å‰°GPUã‚’ä½¿ã‚ã›ã¦ã„ãŸã ãã¾ã™'

USER      RUNNING DESIRED     TO DATE  COMMENT
============================================================================
hidaka          0       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
minami          6       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
takehisa        0       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
fujita          9       5  2022/10/31  ä½¿ã„ã¾ã™
============================================================================
Remaining       1

wandb: Currently logged in as: naoaki. Use `wandb login --relogin` to force relogin
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
tag: scale_len700_start030_layer3
device = cuda
cpu_num = 36
gpu_num = 1

--- data directory check ---
data_root = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/train
mean_std_path = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/mean_std
ckpt_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/check_point/default/lip/scale_len700_start030_layer3
save_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/result/default/train/lip/scale_len700_start030_layer3

--- get datasets ---
load F01_kablab

--- make train dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 3639

--- make validation dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 192
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/outputs/2022-10-04/16-18-38/wandb/run-20221004_161838-cnmxjtlp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scale_len700_start030_layer3_mspec80
wandb: â­ï¸ View project at https://wandb.ai/naoaki/lip2sp_default
wandb: ğŸš€ View run at https://wandb.ai/naoaki/lip2sp_default/runs/cnmxjtlp
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced scale_len700_start030_layer3_mspec80: https://wandb.ai/naoaki/lip2sp_default/runs/cnmxjtlp
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221004_161838-cnmxjtlp/logs
Error executing job with overrides: ['train.debug=False', 'tag=scale_len700_start030_layer3']
Traceback (most recent call last):
  File "train_default.py", line 282, in main
    model = make_model(cfg, device)
  File "train_default.py", line 48, in make_model
    model = Lip2SP(
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/model/model_default.py", line 78, in __init__
    self.encoder = Encoder(
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/model/transformer_remake.py", line 225, in __init__
    self.alpha = nn.Parameter(torch.ones(1.0))
TypeError: ones(): argument 'size' (position 1) must be tuple of ints, not float

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
