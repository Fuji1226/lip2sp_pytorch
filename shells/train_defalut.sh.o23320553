
  - ç¾åœ¨ã®ç ”ç©¶å®¤ã®GPUä½¿ç”¨çŠ¶æ³ã¯ï¼Œ
    ã€Œonsei_check_gpusã€ã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ã§ç¢ºèªã§ãã¾ã™ã€‚
  - è‡ªåˆ†ãŒç¢ºä¿ã—ãŸã„GPUã®æšæ•°ã¯ï¼Œ
    ã€Œonsei_register_desired_gpusã€ã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ã§ç™»éŒ²ã§ãã¾ã™ã€‚
    - GPUã‚’å½“åˆ†ä½¿ã†äºˆå®šãŒãªã„å ´åˆã®ä¾‹ï¼ˆæœŸé™ã‚’9999å¹´12æœˆ31æ—¥ã«è¨­å®šï¼‰
      onsei_register_desired_gpus 0 9999/12/31
    - 8æšç¢ºä¿ã—ãŸã„å ´åˆã®ä¾‹ï¼ˆæœŸé™ã‚’2022å¹´10æœˆ11æ—¥ã«è¨­å®šã—ï¼Œã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã‚‹ï¼‰
      onsei_register_desired_gpus 8 2022/10/11 'ç ”ç©¶ç· åˆ‡ã®ãŸã‚ã™ã¿ã¾ã›ã‚“'
    - æœ€ä½2æšç¢ºä¿ã—ï¼Œç ”ç©¶å®¤å…¨ä½“ã§ä½™ã£ãŸGPUã‚‚ä½¿ã„ãŸã„å ´åˆã®ä¾‹
      onsei_register_desired_gpus 2 2022/10/11 'ä½™å‰°GPUã‚’ä½¿ã‚ã›ã¦ã„ãŸã ãã¾ã™'

USER      RUNNING DESIRED     TO DATE  COMMENT
============================================================================
hidaka          0       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
minami          5       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
takehisa        0       0  9999/12/31  è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆï¼š2022/09/30ã«æœŸé™åˆ‡ã‚Œ
fujita          6       5  2022/10/31  ä½¿ã„ã¾ã™
============================================================================
Remaining       5

wandb: Currently logged in as: naoaki. Use `wandb login --relogin` to force relogin
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
tag: scale_recenter_len700_start040
device = cuda
cpu_num = 36
gpu_num = 1

--- data directory check ---
data_root = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/train
mean_std_path = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/mean_std
ckpt_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/check_point/default/lip/scale_recenter_len700_start040
save_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/result/default/train/lip/scale_recenter_len700_start040

--- get datasets ---
load F01_kablab

--- make train dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 3639

--- make validation dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 192
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/outputs/2022-10-04/14-39-37/wandb/run-20221004_143937-3l1fpc8z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run default_mspec80
wandb: â­ï¸ View project at https://wandb.ai/naoaki/lip2sp_default
wandb: ğŸš€ View run at https://wandb.ai/naoaki/lip2sp_default/runs/3l1fpc8z
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(##### 1 #####
training_method : ss
mixing_prob = 0.5
learning_rate = 0.001
iter start

iter 0/113
iter 1/113
iter 2/113
iter 3/113
iter 4/113
iter 5/113
iter 6/113
iter 7/113
iter 8/113
iter 9/113
iter 10/113
iter 11/113
iter 12/113
iter 13/113
iter 14/113
iter 15/113
iter 16/113
iter 17/113
iter 18/113
iter 19/113
iter 20/113
iter 21/113
iter 22/113
iter 23/113
iter 24/113
iter 25/113
iter 26/113
iter 27/113
iter 28/113
iter 29/113
iter 30/113
iter 31/113
iter 32/113
iter 33/113
iter 34/113
iter 35/113
iter 36/113
iter 37/113
iter 38/113
iter 39/113
iter 40/113
iter 41/113
iter 42/113
iter 43/113
iter 44/113
iter 45/113
iter 46/113
iter 47/113
iter 48/113
iter 49/113
iter 50/113
iter 51/113
iter 52/113
iter 53/113
iter 54/113
iter 55/113
iter 56/113
iter 57/113
iter 58/113
iter 59/113
iter 60/113
iter 61/113
iter 62/113
iter 63/113
iter 64/113
iter 65/113
iter 66/113
iter 67/113
iter 68/113
iter 69/113
iter 70/113
iter 71/113
iter 72/113
iter 73/113
iter 74/113
iter 75/113
iter 76/113
iter 77/113
iter 78/113
iter 79/113
iter 80/113
iter 81/113
iter 82/113
iter 83/113
iter 84/113
iter 85/113
iter 86/113
iter 87/113
iter 88/113
iter 89/113
iter 90/113
iter 91/113
iter 92/113
iter 93/113
iter 94/113
iter 95/113
iter 96/113
iter 97/113
iter 98/113
iter 99/113
iter 100/113
iter 101/113
iter 102/113
iter 103/113
iter 104/113
iter 105/113
iter 106/113
iter 107/113
iter 108/113
iter 109/113
iter 110/113
iter 111/113
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: | 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: | 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.403 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb: train_dec_output_loss â–ˆâ–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train_output_loss â–‡â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: train_dec_output_loss 0.45637
wandb:     train_output_loss 0.38715
wandb: 
wandb: Synced default_mspec80: https://wandb.ai/naoaki/lip2sp_default/runs/3l1fpc8z
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221004_143937-3l1fpc8z/logs
Error executing job with overrides: ['train.debug=False', 'tag=scale_recenter_len700_start040']
Traceback (most recent call last):
  File "train_default.py", line 349, in main
    train_epoch_loss_output, train_epoch_loss_dec_output, train_epoch_loss_feat_add = train_one_epoch(
  File "train_default.py", line 150, in train_one_epoch
    check_mel_default(feature[0], output[0], dec_output[0], cfg, "mel_train", current_time, ckpt_time)
  File "/home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/utils.py", line 329, in check_mel_default
    wandb.log({f"{filename}": wandb.Image(str(save_path / f"{filename}.png"))})
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/wandb/sdk/data_types/image.py", line 143, in __init__
    self._initialize_from_path(data_or_path)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/wandb/sdk/data_types/image.py", line 238, in _initialize_from_path
    self._image = pil_image.open(path)
  File "/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/PIL/Image.py", line 3030, in open
    raise UnidentifiedImageError(
PIL.UnidentifiedImageError: cannot identify image file '/home/usr1/q70261a/lip2sp_pytorch/data_check/default/2022:10:04_14-39-36/mel_train.png'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
