
  - 現在の研究室のGPU使用状況は，
    「onsei_check_gpus」というコマンドで確認できます。
  - 自分が確保したいGPUの枚数は，
    「onsei_register_desired_gpus」というコマンドで登録できます。
    - GPUを当分使う予定がない場合の例（期限を9999年12月31日に設定）
      onsei_register_desired_gpus 0 9999/12/31
    - 8枚確保したい場合の例（期限を2022年10月11日に設定し，コメントを付ける）
      onsei_register_desired_gpus 8 2022/10/11 '研究締切のためすみません'
    - 最低2枚確保し，研究室全体で余ったGPUも使いたい場合の例
      onsei_register_desired_gpus 2 2022/10/11 '余剰GPUを使わせていただきます'

USER      RUNNING DESIRED     TO DATE  COMMENT
============================================================================
hidaka          0       0  9999/12/31  自動コメント：2022/09/30に期限切れ
minami          2       0  9999/12/31  自動コメント：2022/09/30に期限切れ
takehisa        0       0  9999/12/31  自動コメント：2022/09/30に期限切れ
fujita          5       5  2022/10/31  使います
============================================================================
Remaining       9

wandb: Currently logged in as: naoaki. Use `wandb login --relogin` to force relogin
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
device = cuda
cpu_num = 36
gpu_num = 1

--- data directory check ---
data_root = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/train
mean_std_path = /home/usr1/q70261a/dataset/lip/np_files/lip_cropped/mean_std
ckpt_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/check_point/default/lip/scale_recenter_prenet_up_2
save_path = /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/result/default/train/lip/scale_recenter_prenet_up_2

--- get datasets ---
load F01_kablab

--- make train dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 3639

--- make validation dataset ---

get speaker idx
speaker_idx = {'F01_kablab': 0}

load mean std
load F01_kablab
n = 192
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/usr1/q70261a/lip2sp_pytorch_all/lip2sp_920_re/outputs/2022-10-04/10-45-33/wandb/run-20221004_104533-mnkludcs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run default_mspec80
wandb: ⭐️ View project at https://wandb.ai/naoaki/lip2sp_default
wandb: 🚀 View run at https://wandb.ai/naoaki/lip2sp_default/runs/mnkludcs
##### 1 #####
training_method : ss
mixing_prob = 0.5
learning_rate = 0.001
iter start
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
iter 0/113
iter 1/113
iter 2/113
iter 3/113
iter 4/113
iter 5/113
iter 6/113
iter 7/113
iter 8/113
iter 9/113
iter 10/113
iter 11/113
iter 12/113
iter 13/113/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

iter 14/113
iter 15/113
iter 16/113
iter 17/113
iter 18/113
iter 19/113
iter 20/113
iter 21/113
iter 22/113
iter 23/113
iter 24/113
iter 25/113
iter 26/113
iter 27/113
iter 28/113
iter 29/113
iter 30/113
iter 31/113
iter 32/113
iter 33/113
iter 34/113
iter 35/113
iter 36/113
iter 37/113
iter 38/113
iter 39/113
iter 40/113
iter 41/113
iter 42/113
iter 43/113
iter 44/113
iter 45/113
iter 46/113
iter 47/113
iter 48/113
iter 49/113
iter 50/113
iter 51/113
iter 52/113
iter 53/113
iter 54/113
iter 55/113
iter 56/113
iter 57/113
iter 58/113
iter 59/113
iter 60/113
iter 61/113
iter 62/113
iter 63/113
iter 64/113
iter 65/113
iter 66/113
iter 67/113
iter 68/113
iter 69/113
iter 70/113
iter 71/113
iter 72/113
iter 73/113
iter 74/113
iter 75/113
iter 76/113
iter 77/113
iter 78/113
iter 79/113
iter 80/113
iter 81/113
iter 82/113
iter 83/113
iter 84/113
iter 85/113
iter 86/113
iter 87/113
iter 88/113
iter 89/113
iter 90/113
iter 91/113
iter 92/113
iter 93/113
iter 94/113
iter 95/113
iter 96/113
iter 97/113
iter 98/113
iter 99/113
iter 100/113
iter 101/113
iter 102/113
iter 103/113
iter 104/113
iter 105/113
iter 106/113
iter 107/113
iter 108/113
iter 109/113
iter 110/113
iter 111/113
iter 112/113
calc val loss
iter 0/6
iter 1/6
iter 2/6
iter 3/6
iter 4/6
iter 5/6
##### 2 #####
training_method : ss
mixing_prob = 0.5
learning_rate = 0.001
iter start
iter 0/113
iter 1/113
/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
iter 2/113
iter 3/113
iter 4/113
iter 5/113
iter 6/113
iter 7/113
iter 8/113
iter 9/113
iter 10/113
iter 11/113
iter 12/113
iter 13/113
iter 14/113
iter 15/113
iter 16/113
iter 17/113
iter 18/113
iter 19/113
iter 20/113
iter 21/113
iter 22/113
iter 23/113
iter 24/113
iter 25/113
iter 26/113
iter 27/113
iter 28/113
iter 29/113
iter 30/113
iter 31/113
iter 32/113
iter 33/113
iter 34/113
iter 35/113
iter 36/113
iter 37/113
iter 38/113
iter 39/113
iter 40/113
iter 41/113
iter 42/113
iter 43/113
iter 44/113
iter 45/113
iter 46/113
iter 47/113
iter 48/113
iter 49/113
iter 50/113
iter 51/113
iter 52/113
iter 53/113
iter 54/113
iter 55/113
iter 56/113
iter 57/113
iter 58/113
iter 59/113
iter 60/113
iter 61/113
iter 62/113
iter 63/113
iter 64/113
iter 65/113
iter 66/113
iter 67/113
iter 68/113
iter 69/113
iter 70/113
iter 71/113
iter 72/113
iter 73/113
iter 74/113
iter 75/113
iter 76/113
iter 77/113
iter 78/113
iter 79/113
iter 80/113
iter 81/113
iter 82/113
iter 83/113
iter 84/113
iter 85/113
iter 86/113
iter 87/113
iter 88/113
iter 89/113
iter 90/113
iter 91/113
iter 92/113
iter 93/113
iter 94/113
iter 95/113
iter 96/113
iter 97/113
iter 98/113
iter 99/113
iter 100/113
iter 101/113
iter 102/113
iter 103/113
iter 104/113
iter 105/113
iter 106/113
iter 107/113
iter 108/113
iter 109/113
iter 110/113
iter 111/113
iter 112/113
calc val loss
iter 0/6
iter 1/6
iter 2/6
iter 3/6
iter 4/6
iter 5/6
##### 3 #####
training_method : ss
mixing_prob = 0.5
learning_rate = 0.001
iter start
iter 0/113
iter 1/113
iter 2/113
iter 3/113
iter 4/113
iter 5/113
iter 6/113
iter 7/113
iter 8/113
iter 9/113
iter 10/113
iter 11/113
iter 12/113
iter 13/113/home/usr1/q70261a/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 9, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

iter 14/113
iter 15/113
iter 16/113
iter 17/113
iter 18/113
iter 19/113
iter 20/113
iter 21/113
iter 22/113
iter 23/113
iter 24/113
iter 25/113
iter 26/113
iter 27/113
iter 28/113
iter 29/113
iter 30/113
iter 31/113
iter 32/113
iter 33/113
iter 34/113
iter 35/113
iter 36/113
iter 37/113
iter 38/113
iter 39/113
iter 40/113
iter 41/113
iter 42/113
iter 43/113
iter 44/113
iter 45/113
iter 46/113
iter 47/113
iter 48/113
iter 49/113
iter 50/113
iter 51/113
iter 52/113
iter 53/113
iter 54/113
iter 55/113
iter 56/113
iter 57/113
iter 58/113
iter 59/113
iter 60/113
iter 61/113
iter 62/113
iter 63/113
iter 64/113
iter 65/113
iter 66/113
iter 67/113
iter 68/113
iter 69/113
iter 70/113
iter 71/113
iter 72/113
iter 73/113
iter 74/113
iter 75/113
iter 76/113
iter 77/113
iter 78/113
iter 79/113
iter 80/113
iter 81/113
iter 82/113
iter 83/113
iter 84/113
iter 85/113
iter 86/113
iter 87/113
iter 88/113
iter 89/113
iter 90/113
iter 91/113
iter 92/113
iter 93/113
iter 94/113
iter 95/113
iter 96/113
iter 97/113
iter 98/113
iter 99/113
iter 100/113
iter 101/113
iter 102/113
iter 103/113
iter 104/113
iter 105/113
iter 106/113
iter 107/113
iter 108/113
iter 109/113
iter 110/113
iter 111/113
iter 112/113
calc val loss
iter 0/6
iter 1/6
iter 2/6
iter 3/6
iter 4/6
iter 5/6
##### 4 #####
training_method : ss
mixing_prob = 0.5
learning_rate = 0.001
iter start
Terminated
